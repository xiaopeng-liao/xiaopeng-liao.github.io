{
  "name": "Xiaopeng-liao.GitHub.io",
  "tagline": "Xiaopeng Liao's blog",
  "body": "# Welcome to Xiaopeng Liao's blog\r\nI am interesting in software design in general, love to try new ideas, writing software for different systems (embedded, desktop, server), laying out circuit boards, implementing algorithms... and currently into big data and machine learning technologies.\r\n\r\n## Spark Learning\r\nHere I will note down some learning encounter in Spark\r\n### [2016-09-21] How Spark handles the locality of the task\r\nI have been always wondering how Spark handles the locality of the task, how does it connects with RDD method \r\n` protected def getPreferredLocations(split: Partition): Seq[String] = Nil`\r\nIt turn out to be that when there is an operation on RDD, the DAG scheduler will look at the operation and then create a taskset, then submit to task scheduler, during sumbit, it will look at RDD's preferred location and put in different queue, later the scheduler will then assign node to execute the task based on this this. \r\n\r\nThe code looks like\r\n```\r\n   /**\r\n   * Dequeue a pending task for a given node and return its index and locality level.\r\n   * Only search for tasks matching the given locality constraint.\r\n   *\r\n   * @return An option containing (task index within the task set, locality, is speculative?)\r\n   */\r\n  private def dequeueTask(execId: String, host: String, maxLocality: TaskLocality.Value)\r\n    : Option[(Int, TaskLocality.Value, Boolean)] =\r\n  {\r\n    for (index <- dequeueTaskFromList(execId, getPendingTasksForExecutor(execId))) {\r\n      return Some((index, TaskLocality.PROCESS_LOCAL, false))\r\n    }\r\n\r\n    if (TaskLocality.isAllowed(maxLocality, TaskLocality.NODE_LOCAL)) {\r\n      for (index <- dequeueTaskFromList(execId, getPendingTasksForHost(host))) {\r\n        return Some((index, TaskLocality.NODE_LOCAL, false))\r\n      }\r\n    }\r\n\r\n    if (TaskLocality.isAllowed(maxLocality, TaskLocality.NO_PREF)) {\r\n      // Look for noPref tasks after NODE_LOCAL for minimize cross-rack traffic\r\n      for (index <- dequeueTaskFromList(execId, pendingTasksWithNoPrefs)) {\r\n        return Some((index, TaskLocality.PROCESS_LOCAL, false))\r\n      }\r\n    }\r\n\r\n    if (TaskLocality.isAllowed(maxLocality, TaskLocality.RACK_LOCAL)) {\r\n      for {\r\n        rack <- sched.getRackForHost(host)\r\n        index <- dequeueTaskFromList(execId, getPendingTasksForRack(rack))\r\n      } {\r\n        return Some((index, TaskLocality.RACK_LOCAL, false))\r\n      }\r\n    }\r\n\r\n    if (TaskLocality.isAllowed(maxLocality, TaskLocality.ANY)) {\r\n      for (index <- dequeueTaskFromList(execId, allPendingTasks)) {\r\n        return Some((index, TaskLocality.ANY, false))\r\n      }\r\n    }\r\n\r\n    // find a speculative task if all others tasks have been scheduled\r\n    dequeueSpeculativeTask(execId, host, maxLocality).map {\r\n      case (taskIndex, allowedLocality) => (taskIndex, allowedLocality, true)}\r\n  }\r\n```\r\n\r\n### [2016-09-27] Looking into Spark neural network (MultilayerPerceptronClassifier) implementation\r\nI was always curious on how Spark achieves the distributed learning with its RDD. And finally I got some time to look into this interesting topic, and it looks like Spark does the random sample first from the data set, then broadcast the current weight, then for each sample it calculates the gradient and the loss, then it sums up all the gradients and loss for this subset, divide by sample number to get an average of both loss and gradient. Then as next step, it uses LBFGS algorithm to update the weights. Normally the LBFGS is a iterative update, however, in Spark, it is implemented by using average of samples to perform on update, so I guess it is time to get into some paper on effectiveness of this type of method :)",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}